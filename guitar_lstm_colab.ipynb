{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pedalboard\n",
      "  Downloading pedalboard-0.3.12-cp38-cp38-macosx_10_9_universal2.whl (6.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.3 MB 5.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/anaconda3/lib/python3.8/site-packages (from pedalboard) (1.20.3)\n",
      "Installing collected packages: pedalboard\n",
      "Successfully installed pedalboard-0.3.12\n"
     ]
    }
   ],
   "source": [
    "#!pip install tensorflow\n",
    "#!pip install pedalboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO USE: \n",
    "#    1. Upload your input and output wav files to the current directory in Colab\n",
    "#    2. Edit the USER INPUTS section to point to your wav files, and choose a\n",
    "#         model name, and number of epochs for training. \n",
    "#    3. Run each section of code. The trained models and output wav files will be \n",
    "#         added to the \"models\" directory.\n",
    "#\n",
    "#     Note: Tested on CPU and GPU runtimes.\n",
    "#     Note: Uses MSE for loss calculation instead of Error to Signal with Pre-emphasis filter\n",
    "\n",
    "import soundfile as sf\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Conv1D, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.activations import tanh, elu, relu\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import os\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import h5py\n",
    "import librosa\n",
    "import pkg_resources\n",
    "#pkg_resources.require(\"numpy==`1.19\")  # modified to use specific numpy\n",
    "from pedalboard import (\n",
    "    Pedalboard,\n",
    "    Convolution,\n",
    "    Compressor,\n",
    "    Chorus,\n",
    "    Distortion,\n",
    "    Gain,\n",
    "    Reverb,\n",
    "    Limiter,\n",
    "    LadderFilter,\n",
    "    Phaser,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nin_file = \\'data/ts9_test1_in_FP32.wav\\'\\nout_file = \\'data/ts9_test1_out_FP32.wav\\'\\nepochs = 1\\n\\ntrain_mode = 0     # 0 = speed training, \\n                   # 1 = accuracy training \\n                   # 2 = extended training\\n\\ninput_size = 150 \\n\\nif not os.path.exists(\\'models/\\'+name):\\n    os.makedirs(\\'models/\\'+name)\\nelse:\\n    print(\"A model with the same name already exists. Please choose a new name.\")\\n    exit\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EDIT THIS SECTION FOR USER INPUTS\n",
    "#\n",
    "\n",
    "name = 'Michas_Chorus_Test'\n",
    "'''\n",
    "in_file = 'data/ts9_test1_in_FP32.wav'\n",
    "out_file = 'data/ts9_test1_out_FP32.wav'\n",
    "epochs = 1\n",
    "\n",
    "train_mode = 0     # 0 = speed training, \n",
    "                   # 1 = accuracy training \n",
    "                   # 2 = extended training\n",
    "\n",
    "input_size = 150 \n",
    "\n",
    "if not os.path.exists('models/'+name):\n",
    "    os.makedirs('models/'+name)\n",
    "else:\n",
    "    print(\"A model with the same name already exists. Please choose a new name.\")\n",
    "    exit\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 44100\n",
    "# normalize data to loudest signal\n",
    "def normalize(data):\n",
    "    data_max = max(data)\n",
    "    data_min = min(data)\n",
    "    data_norm = max(data_max,abs(data_min))\n",
    "    return data / data_norm\n",
    "\n",
    "def error_to_signal(y_true, y_pred): \n",
    "    \"\"\"\n",
    "    Error to signal ratio with pre-emphasis filter:\n",
    "    \"\"\"\n",
    "    y_true, y_pred = pre_emphasis_filter(y_true), pre_emphasis_filter(y_pred)\n",
    "    return K.sum(tf.pow(y_true - y_pred, 2), axis=0) / (K.sum(tf.pow(y_true, 2), axis=0) + 1e-10)\n",
    "\n",
    "# add fadeout with length samples   \n",
    "def apply_fadeout(audio, length):\n",
    "    # convert to audio indices (samples)\n",
    "    #length = int(duration*sr)\n",
    "    end = len(audio)\n",
    "    start = end - length\n",
    "\n",
    "    # compute fade out curve\n",
    "    # linear fade\n",
    "    fade_curve = np.linspace(1.0, 0.0, length)\n",
    "    audio[start:end] = audio[start:end] *fade_curve\n",
    "    return audio\n",
    "\n",
    "\n",
    "\n",
    "# loads guitar signals, cuts them to 1 sec and rewrties them with fade out and normalized\n",
    "for input in range(1, 101, 1):\n",
    "    x, samplerate = librosa.load('Trial_Data/GuitSigClean_Monophon/Monophon (%d).wav' %input, mono=True, sr=fs, duration=1.0)          \n",
    "    x = apply_fadeout(normalize(x),100)\n",
    "    with sf.SoundFile('Trial_Data/GuitSigClean_1/Monophon_1sec_(%d).wav' %(input), 'w', samplerate=fs, channels=len(x.shape)) as f:\n",
    "        f.write(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 44100)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# writing pedalboard processed audio sorted by effect type\n",
    "# reading processed audio and append to dry and wet array\n",
    "fx_list = [Chorus(), Phaser(), Distortion()]\n",
    "fx_names = ['Chorus', 'Phaser', 'Distortion']\n",
    "dry = []\n",
    "wet = []\n",
    "effect_indices = []\n",
    "for fx in range(0, 3, 1): # 0 = Chorus, 1 = Phaser, 2 = Distortion\n",
    "    board = Pedalboard([fx_list[fx]], sample_rate=fs)\n",
    "    for input in range(1, 101, 1):\n",
    "        x, samplerate = librosa.load('Trial_Data//GuitSigClean_1/Monophon_1sec_(%d).wav' %input, mono=True, sr=fs, duration=1.0)\n",
    "        x = apply_fadeout(normalize(x),100)\n",
    "        output = board(x)\n",
    "        output = apply_fadeout(output, 100)\n",
    "        dry.append(x)\n",
    "        wet.append(output)\n",
    "        effect_indices.append(fx)\n",
    "        with sf.SoundFile('Trial_Data/GuitSig_with_%s/Monophon_with_%s (%d).wav' %(fx_names[fx],fx_names[fx],input), 'w', samplerate=fs, channels=len(output.shape)) as f:\n",
    "            f.write(output)\n",
    "\n",
    "dry = np.array(dry)\n",
    "wet = np.array(wet)\n",
    "effect_indices = np.array(effect_indices)\n",
    "dataset_dry = tf.data.Dataset.from_tensor_slices(dry)\n",
    "dataset_wet = tf.data.Dataset.from_tensor_slices(wet)\n",
    "dataset_effect_indices = tf.data.Dataset.from_tensor_slices(effect_indices)\n",
    "wet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowArray(Sequence):\n",
    "        \n",
    "    def __init__(self, x, y, window_len, batch_size=32):\n",
    "        self.x = x\n",
    "        self.y = y[window_len-1:] \n",
    "        self.window_len = window_len\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (len(self.x) - self.window_len +1) // self.batch_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x_out = np.stack([self.x[idx: idx+self.window_len] for idx in range(index*self.batch_size, (index+1)*self.batch_size)])\n",
    "        y_out = self.y[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        return x_out, y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 37, 16)            208       \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 4, 16)             3088      \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 36)                7632      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 37        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,965\n",
      "Trainable params: 10,965\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "861/861 [==============================] - 143s 164ms/step - loss: 0.0146 - error_to_signal: 40.2947 - val_loss: 0.0113 - val_error_to_signal: 0.8296\n",
      "Epoch 2/3\n",
      "861/861 [==============================] - 155s 179ms/step - loss: 0.0139 - error_to_signal: 227.2724 - val_loss: 0.0114 - val_error_to_signal: 2.3004\n",
      "Epoch 3/3\n",
      "861/861 [==============================] - 147s 171ms/step - loss: 0.0131 - error_to_signal: 26.4606 - val_loss: 0.0115 - val_error_to_signal: 2.6076\n",
      "Running prediction..\n"
     ]
    }
   ],
   "source": [
    "def pre_emphasis_filter(x, coeff=0.95):\n",
    "    return tf.concat([x, x - coeff * x], 1)\n",
    "    \n",
    "def error_to_signal(y_true, y_pred): \n",
    "    \"\"\"\n",
    "    Error to signal ratio with pre-emphasis filter:\n",
    "    \"\"\"\n",
    "    y_true, y_pred = pre_emphasis_filter(y_true), pre_emphasis_filter(y_pred)\n",
    "    return K.sum(tf.pow(y_true - y_pred, 2), axis=0) / (K.sum(tf.pow(y_true, 2), axis=0) + 1e-10)\n",
    "    \n",
    "def save_wav(name, data):\n",
    "    wavfile.write(name, 44100, data.flatten().astype(np.float32))\n",
    "\n",
    "def normalize(data):\n",
    "    data_max = max(data)\n",
    "    data_min = min(data)\n",
    "    data_norm = max(data_max,abs(data_min))\n",
    "    return data / data_norm\n",
    "\n",
    "\n",
    "'''This is a similar Tensorflow/Keras implementation of the LSTM model from the paper:\n",
    "    \"Real-Time Guitar Amplifier Emulation with Deep Learning\"\n",
    "    https://www.mdpi.com/2076-3417/10/3/766/htm\n",
    "\n",
    "    Uses a stack of two 1-D Convolutional layers, followed by LSTM, followed by \n",
    "    a Dense (fully connected) layer. Three preset training modes are available, \n",
    "    with further customization by editing the code. A Sequential tf.keras model \n",
    "    is implemented here.\n",
    "\n",
    "    Note: RAM may be a limiting factor for the parameter \"input_size\". The wav data\n",
    "      is preprocessed and stored in RAM, which improves training speed but quickly runs out\n",
    "      if using a large number for \"input_size\".  Reduce this if you are experiencing\n",
    "      RAM issues. \n",
    "    \n",
    "    --training_mode=0   Speed training (default)\n",
    "    --training_mode=1   Accuracy training\n",
    "    --training_mode=2   Extended training (set max_epochs as desired, for example 50+)\n",
    "'''\n",
    "\n",
    "batch_size = 20 #4096 \n",
    "test_size = 0.2\n",
    "train_mode = 0\n",
    "input_size = 75\n",
    "epochs = 3\n",
    "\n",
    "if train_mode == 0:         # Speed Training\n",
    "    learning_rate = 0.01 \n",
    "    conv1d_strides = 12    \n",
    "    conv1d_filters = 16\n",
    "    hidden_units = 36\n",
    "elif train_mode == 1:       # Accuracy Training (~10x longer than Speed Training)\n",
    "    learning_rate = 0.01 \n",
    "    conv1d_strides = 4\n",
    "    conv1d_filters = 36\n",
    "    hidden_units= 64\n",
    "else:                       # Extended Training (~60x longer than Accuracy Training)\n",
    "    learning_rate = 0.0005 \n",
    "    conv1d_strides = 3\n",
    "    conv1d_filters = 36\n",
    "    hidden_units= 96\n",
    "\n",
    "\n",
    "# Create Sequential Model ###########################################\n",
    "clear_session()\n",
    "model = Sequential()\n",
    "model.add(Conv1D(conv1d_filters, 12,strides=conv1d_strides, activation=None, padding='same',input_shape=(input_size,1)))\n",
    "model.add(Conv1D(conv1d_filters, 12,strides=conv1d_strides, activation=None, padding='same'))\n",
    "model.add(LSTM(hidden_units))\n",
    "model.add(Dense(1, activation=None))\n",
    "model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse', metrics=[error_to_signal])\n",
    "model.summary()\n",
    "\n",
    "# Load and Preprocess Data ###########################################\n",
    "#in_rate, in_data = wavfile.read(in_file)\n",
    "#out_rate, out_data = wavfile.read(out_file)\n",
    "\n",
    "\n",
    "# Chorus\n",
    "effect_indices[:100]\n",
    "X_all = dry[:100].astype(np.float32).flatten()  \n",
    "X_all = normalize(X_all).reshape(len(X_all),1)   \n",
    "y_all = wet[:100].astype(np.float32).flatten() \n",
    "y_all = normalize(y_all).reshape(len(y_all),1)\n",
    "\n",
    "train_examples = int(len(X_all)*0.8)\n",
    "train_arr = WindowArray(X_all[:train_examples], y_all[:train_examples], input_size, batch_size=batch_size)\n",
    "val_arr = WindowArray(X_all[train_examples:], y_all[train_examples:], input_size, batch_size=batch_size)\n",
    "\n",
    "# Train Model ###################################################\n",
    "history = model.fit(train_arr, validation_data=val_arr, epochs=epochs, shuffle=True)    \n",
    "model.save('models/'+name+'/'+name+'.h5')\n",
    "\n",
    "# Run Prediction #################################################\n",
    "print(\"Running prediction..\")\n",
    "\n",
    "# Get the last 20% of the wav data to run prediction and plot results\n",
    "y_the_rest, y_last_part = np.split(y_all, [int(len(y_all)*.8)])\n",
    "x_the_rest, x_last_part = np.split(X_all, [int(len(X_all)*.8)])\n",
    "y_test = y_last_part[input_size-1:] \n",
    "test_arr = WindowArray(x_last_part, y_last_part, input_size, batch_size = batch_size)\n",
    "\n",
    "prediction = model.predict(test_arr)\n",
    "\n",
    "save_wav('models/'+name+'/y_pred.wav', prediction)\n",
    "save_wav('models/'+name+'/x_test.wav', x_last_part)\n",
    "save_wav('models/'+name+'/y_test.wav', y_test)\n",
    "\n",
    "# Add additional data to the saved model (like input_size)\n",
    "filename = 'models/'+name+'/'+name+'.h5'\n",
    "f = h5py.File(filename, 'a')\n",
    "grp = f.create_group(\"info\")\n",
    "dset = grp.create_dataset(\"input_size\", (1,), dtype='int16')\n",
    "dset[0] = input_size\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
